{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `U-Net: Convolutional Networks for Biomedical Image Segmentation`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![UNet Architecture](../u-net-architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2x(in_channels: int, out_channels: int) -> nn.Sequential:\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "def conv_trans2x(in_channels: int, out_channels: int) -> nn.Sequential:\n",
    "    convTrans = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2)\n",
    "    return convTrans\n",
    "\n",
    "def crop_img(tensor: torch.Tensor, target_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    target_size = target_tensor.size()[2] # Second index, Height\n",
    "    tensor_size = tensor.size()[2]\n",
    "    delta = tensor_size - target_size\n",
    "    delta = delta // 2\n",
    "    return tensor[:, :, delta:tensor_size - delta, delta:tensor_size - delta]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Implementation with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.conv_c = [\n",
    "            [1, 64],\n",
    "            [64, 128],\n",
    "            [128, 256],\n",
    "            [256, 512],\n",
    "            [512, 1024]\n",
    "        ]\n",
    "\n",
    "        self.reverse_conv_c = [conv_c[::-1] for conv_c in self.conv_c[:0:-1]] # [[1024, 512], [512, 256], [256, 128], [128, 64]]\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.down_convs = [conv2x(channel[0], channel[1]) for channel in self.conv_c]\n",
    "        self.up_convs = [[conv_trans2x(channel[0], channel[1]), conv2x(channel[0], channel[1])] for channel in self.reverse_conv_c]\n",
    "\n",
    "        self.out = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=2,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Encoder (Downward)\n",
    "        # Batch Size, Channel, Height, Width\n",
    "        # Img size: [1, 1, 572, 572]\n",
    "\n",
    "        x1 = self.down_convs[0](img) # [1, 64, 568, 568]\n",
    "\n",
    "        x2 = self.max_pool(x1) # [1, 64, 284, 284]\n",
    "        x3 = self.down_convs[1](x2) # [1, 128, 280, 280]\n",
    "\n",
    "        x4 = self.max_pool(x3) # [1, 128, 140, 140]\n",
    "        x5 = self.down_convs[2](x4) # [1, 256, 136, 136]\n",
    "\n",
    "        x6 = self.max_pool(x5) # [1, 256, 68, 68]\n",
    "        x7 = self.down_convs[3](x6) # [1, 512, 64, 64]\n",
    "\n",
    "        x8 = self.max_pool(x7) # [1, 512, 32, 32]\n",
    "        x9 = self.down_convs[4](x8) # [1, 1024, 28, 28]\n",
    "\n",
    "\n",
    "        # Decoder (Upward)\n",
    "        x = self.up_convs[0][0](x9) # [1, 512, 56, 56]\n",
    "        y = crop_img(x7, x) # [1, 512, 56, 56]\n",
    "        x = self.up_convs[0][1](torch.cat([x, y], 1)) # [1, 512, 52, 52]\n",
    "\n",
    "        x = self.up_convs[1][0](x) # [1, 256, 104, 104]\n",
    "        y = crop_img(x5, x) # [1, 256, 104, 104]\n",
    "        x = self.up_convs[1][1](torch.cat([x, y], 1)) # [1, 256, 100, 100]\n",
    "\n",
    "        x = self.up_convs[2][0](x) # [1, 128, 200, 200]\n",
    "        y = crop_img(x3, x) # [1, 128, 200, 200]\n",
    "        x = self.up_convs[2][1](torch.cat([x, y], 1)) # [1, 128, 196, 196]\n",
    "\n",
    "        x = self.up_convs[3][0](x) # [1, 64, 392, 392]\n",
    "        y = crop_img(x1, x) # [1, 64, 392, 392]\n",
    "        x = self.up_convs[3][1](torch.cat([x, y], 1)) # [1, 64, 388, 388]\n",
    "\n",
    "        x = self.out(x) # [1, 2, 388, 388]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0520, 0.0471, 0.0536,  ..., 0.0474, 0.0412, 0.0463],\n",
      "          [0.0484, 0.0485, 0.0556,  ..., 0.0482, 0.0504, 0.0503],\n",
      "          [0.0488, 0.0547, 0.0505,  ..., 0.0475, 0.0454, 0.0451],\n",
      "          ...,\n",
      "          [0.0485, 0.0485, 0.0497,  ..., 0.0478, 0.0484, 0.0427],\n",
      "          [0.0501, 0.0495, 0.0444,  ..., 0.0513, 0.0541, 0.0505],\n",
      "          [0.0515, 0.0463, 0.0527,  ..., 0.0504, 0.0522, 0.0508]],\n",
      "\n",
      "         [[0.1161, 0.1126, 0.1167,  ..., 0.1156, 0.1171, 0.1164],\n",
      "          [0.1163, 0.1153, 0.1147,  ..., 0.1180, 0.1162, 0.1136],\n",
      "          [0.1154, 0.1164, 0.1148,  ..., 0.1163, 0.1178, 0.1163],\n",
      "          ...,\n",
      "          [0.1176, 0.1167, 0.1154,  ..., 0.1147, 0.1136, 0.1148],\n",
      "          [0.1201, 0.1134, 0.1154,  ..., 0.1144, 0.1158, 0.1130],\n",
      "          [0.1154, 0.1170, 0.1159,  ..., 0.1166, 0.1147, 0.1158]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "image = torch.rand((1, 1, 572, 572))\n",
    "model = UNet()\n",
    "print(model(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd1362c4d48ceefa086e267f69e96a92118c5f3371ed670ed2bc5bbe415d9db8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
